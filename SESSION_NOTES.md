# ANDI Session Notes - July 14, 2025

## Session Summary
Successfully integrated Ollama local LLM server with Meta Llama models into the ANDI platform for privacy-focused AI inference.

## âœ… Completed This Session

### 1. Ollama Infrastructure Setup
- **Location**: `app/open-llm-app/` directory
- **Docker Compose**: GPU + CPU profiles with health checks
- **Makefile**: Complete management system with 12+ commands
- **Environment Config**: `.env.example` with comprehensive settings

### 2. Meta Llama Model Integration  
- **Llama 3.1 8B** â†’ `andi-ciq-analyzer` (Deep classroom analysis)
- **Llama 3.1 7B Instruct** â†’ `andi-coach` (Teacher coaching & recommendations)
- **Llama 3.2 3B** â†’ `andi-realtime` (Fast real-time processing)
- **Automated Setup**: `make setup-models` script downloads and configures everything

### 3. ANDI Stack Integration
- **start-andi.sh**: Added Ollama as service with GPU/CPU auto-detection
- **Status Monitoring**: Health checks and service status reporting
- **Default Services**: Now includes `database web-app langflow ollama`

### 4. Langflow Integration
- **Environment Variables**: Ollama endpoints configured in Langflow
- **Custom Components**: `ollama_andi_component.py` with specialized ANDI classes
- **Sample Flows**: Ready-to-import JSON flows for CIQ analysis and teacher coaching

## ğŸ¯ Key Benefits Achieved
- **Privacy-First**: All teacher/student AI analysis stays local
- **Cost Control**: No per-token charges for LLM inference  
- **FERPA Compliance**: Educational data never leaves local infrastructure
- **Hybrid Approach**: Use Ollama for sensitive data, external APIs for general tasks
- **Performance**: GPU acceleration with CPU fallback

## ğŸ“‹ Next Session Priorities

### Immediate Next Steps (Ready to implement)
1. **Test Complete Stack**:
   ```bash
   ./start-andi.sh --detached
   cd app/open-llm-app && make setup-models  # Downloads 30+ GB
   ```

2. **Langflow Flow Development**:
   - Import sample flows into Langflow IDE (http://localhost:7860)
   - Test CIQ analysis with real classroom transcripts
   - Validate teacher coaching recommendations
   - Create additional flows for specific ANDI use cases

3. **Web App Integration**:
   - Add Ollama model selection to recording analysis UI
   - Create local vs. external AI toggle in settings
   - Update API routes to support Ollama endpoints
   - Add model status indicators in dashboard

### Medium-Term Development
4. **Model Fine-tuning**:
   - Collect ANDI-specific training data
   - Fine-tune models for educational context
   - Optimize prompts for CIQ framework
   - Validate accuracy against educational research

5. **Performance Optimization**:
   - Model caching and pre-loading strategies
   - GPU memory management for multiple models
   - Response time optimization for real-time use cases
   - Load balancing for high-traffic scenarios

6. **Production Readiness**:
   - Security hardening for local LLM deployment
   - Monitoring and alerting for model performance
   - Backup and recovery procedures for model data
   - Documentation for IT administrators

## ğŸ”§ Technical Architecture

### Current Stack
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web App       â”‚    â”‚    Langflow      â”‚    â”‚     Ollama      â”‚    â”‚  Meta Llama     â”‚
â”‚   (Next.js)     â”‚â—„â”€â”€â–ºâ”‚  (AI Workflows)  â”‚â—„â”€â”€â–ºâ”‚  (Local LLMs)   â”‚â—„â”€â”€â–ºâ”‚   Models        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚    â”‚   ClickHouse     â”‚    â”‚   Docker        â”‚    â”‚  Local Storage  â”‚
â”‚   (App Data)    â”‚    â”‚  (Analytics)     â”‚    â”‚  (Containers)   â”‚    â”‚  (Model Files)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### API Endpoints
- **Ollama API**: http://localhost:11434/api/generate
- **Langflow**: http://localhost:7860 (includes Ollama components)
- **Web UI** (optional): http://localhost:8080

### Model Specifications
| Model | Size | Context | Use Case | Performance |
|-------|------|---------|----------|-------------|
| andi-ciq-analyzer | 4.7GB | 4096 tokens | CIQ Analysis | Slow, Deep |
| andi-coach | 4.1GB | 3072 tokens | Teacher Coaching | Medium |
| andi-realtime | 2.0GB | 2048 tokens | Real-time Processing | Fast |

## ğŸ“ File Locations

### Ollama Setup
- **Main Directory**: `app/open-llm-app/`
- **Docker Config**: `app/open-llm-app/docker-compose.yml`
- **Management**: `app/open-llm-app/Makefile`
- **Setup Script**: `app/open-llm-app/scripts/setup-andi-models.sh`

### Langflow Integration  
- **Custom Component**: `app/Langflow/custom_components/ollama_andi_component.py`
- **Sample Flows**: `app/Langflow/flows/ciq_analysis_ollama.json`
- **Environment**: `app/Langflow/.env.example` (updated with Ollama config)

### Startup Integration
- **Main Script**: `start-andi.sh` (updated with Ollama service)
- **Documentation**: `CLAUDE.md` (updated with current stack info)

## ğŸš€ Getting Started Commands

```bash
# Start full ANDI stack
./start-andi.sh --detached

# Download and setup Ollama models (one-time, ~30GB download)
cd app/open-llm-app
make setup-models

# Test Ollama integration
make test
make health

# Access services
# Web App: http://localhost:3000
# Langflow: http://localhost:7860  
# Ollama API: http://localhost:11434
# Ollama Web UI: http://localhost:8080 (if started with webui profile)
```

## ğŸ¯ Session Goals Achieved
âœ… Complete local LLM infrastructure  
âœ… Meta Llama model integration  
âœ… Privacy-first AI architecture  
âœ… Langflow workflow integration  
âœ… ANDI stack orchestration  
âœ… Documentation and setup automation  

**Ready for next session**: Testing, flow development, and web app integration.